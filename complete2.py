import streamlit as st
import os
import pandas as pd
import json
from datetime import datetime
from reportlab.lib.pagesizes import letter, A4
from reportlab.pdfgen import canvas
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from io import BytesIO
from dotenv import load_dotenv
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# ====== í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° (ì„ íƒì‚¬í•­) ======
load_dotenv()

# ====== ê¸°ë³¸ ì„¤ì • ======
st.set_page_config(page_title="ì™¸êµ­ì–´ ê¸°ì‚¬ í•™ìŠµ ë„ìš°ë¯¸", layout="wide")

# ====== ìƒë‹¨ í—¤ë” (ì œëª© + ì»¤í”¼ í›„ì› ë²„íŠ¼) ======
col1, col2 = st.columns([3, 1])

with col1:
    st.title("NEWStudy")

with col2:
    st.markdown("### ")  # ê³µê°„ ì¡°ì •
    if st.button("â˜• Give me a coffee!", type="secondary"):
        st.balloons()
        st.success("ê°ì‚¬í•©ë‹ˆë‹¤! í›„ì› ë§í¬ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤ â˜•")
        # ì‹¤ì œ í›„ì› ë§í¬ë¡œ ì—°ê²°í•˜ë ¤ë©´:
        # st.markdown("[â˜• Buy me a coffee](https://buymeacoffee.com/your-username)")
        # ë˜ëŠ” í† ìŠ¤, ì¹´ì¹´ì˜¤í˜ì´ ë“±ì˜ ë§í¬ ì‚¬ìš© ê°€ëŠ¥

# ====== ì„œë¹„ìŠ¤ ì„¤ëª… ======
st.markdown("---")
st.markdown("""
### ğŸŒ ì „ ì„¸ê³„ ë‰´ìŠ¤Â·ê¸°ì‚¬ë¡œ ì™¸êµ­ì–´ë¥¼ ì‰½ê³  ì¬ë¯¸ìˆê²Œ í•™ìŠµí•˜ì„¸ìš”!

ì´ ì„œë¹„ìŠ¤ëŠ” ì™¸êµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì›¹ ê¸°ë°˜ í•™ìŠµ ë„êµ¬ì…ë‹ˆë‹¤.  
ì›í•˜ëŠ” ë‰´ìŠ¤Â·ê¸°ì‚¬ URLë§Œ ì…ë ¥í•˜ë©´ ì•„ë˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

**ğŸ“– ë¬¸ì¥ë³„ ë²ˆì—­**  
ê¸°ì‚¬ ì›ë¬¸ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ê³ ,  
ì„ íƒí•œ ì–¸ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­ì„ ì œê³µí•©ë‹ˆë‹¤.

**ğŸ“š í•µì‹¬ ë‹¨ì–´ì¥ ìƒì„±**  
ìˆ«ì, ê´€ì‚¬, ì ‘ì†ì‚¬ ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë¥¼ ì œì™¸í•˜ê³   
ì£¼ìš” ì–´íœ˜ë§Œ ë½‘ì•„ ë‹¨ì–´Â·ëœ» í˜•íƒœì˜ ë‹¨ì–´ì¥ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.
""")
st.markdown("---")

# ====== API í‚¤ ì…ë ¥ ì„¹ì…˜ ======
st.markdown("### ğŸ”‘ API í‚¤ ì„¤ì •")
col1, col2 = st.columns([3, 1])

with col1:
    api_key = st.text_input(
        "Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        type="password",
        placeholder="ì—¬ê¸°ì— API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
        help="API í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ë˜ë©° ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )

with col2:
    st.markdown("#### ")  # ê³µê°„ ì¡°ì •
    st.markdown("[ğŸ”— API í‚¤ ë°œê¸‰ë°›ê¸°](https://aistudio.google.com/app/apikey?hl=ko)")

if not api_key:
    st.warning("âš ï¸ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ë ¤ë©´ Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    st.info("ğŸ’¡ ìœ„ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë¬´ë£Œë¡œ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”!")
    st.stop()

# ====== LLM ì—°ê²° (ì‚¬ìš©ì ì…ë ¥ API í‚¤ ì‚¬ìš©) ======
try:
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        google_api_key=api_key
    )
    st.success("âœ… API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    st.error(f"âŒ API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.info("ğŸ’¡ API í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ====== ë‚œì´ë„ ê³„ì‚° í•¨ìˆ˜ ======
def calculate_difficulty(sentences):
    """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚œì´ë„ë¥¼ A1~C2ë¡œ ê³„ì‚°"""
    if not sentences:
        return "A1"
    
    total_words = 0
    complex_words = 0
    
    for sentence in sentences:
        words = sentence.split()
        total_words += len(words)
        complex_words += len([w for w in words if len(w) > 7])
    
    if total_words == 0:
        return "A1"
    
    avg_sentence_length = total_words / len(sentences)
    complex_ratio = complex_words / total_words
    
    # ë‚œì´ë„ ê³„ì‚° ë¡œì§
    if avg_sentence_length < 10 and complex_ratio < 0.1:
        return "A1"
    elif avg_sentence_length < 15 and complex_ratio < 0.15:
        return "A2"
    elif avg_sentence_length < 20 and complex_ratio < 0.2:
        return "B1"
    elif avg_sentence_length < 25 and complex_ratio < 0.25:
        return "B2"
    elif avg_sentence_length < 30 and complex_ratio < 0.3:
        return "C1"
    else:
        return "C2"

# ====== PDF ìƒì„± í•¨ìˆ˜ ======
def create_pdf(sentences, translations, topic):
    """ë¬¸ì¥ê³¼ ë²ˆì—­ì„ PDFë¡œ ìƒì„± (í•œê¸€ í°íŠ¸ ì§€ì›)"""
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    
    # í•œê¸€ í°íŠ¸ ë“±ë¡ ì‹œë„
    try:
        # Windows ì‹œìŠ¤í…œ í°íŠ¸ ê²½ë¡œë“¤
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
            "C:/Windows/Fonts/gulim.ttc",   # êµ´ë¦¼
            "C:/Windows/Fonts/batang.ttc",  # ë°”íƒ•
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",  # macOS
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Linux
        ]
        
        font_registered = False
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    pdfmetrics.registerFont(TTFont('Korean', font_path))
                    font_registered = True
                    break
                except:
                    continue
        
        # ìŠ¤íƒ€ì¼ ì„¤ì •
        styles = getSampleStyleSheet()
        
        if font_registered:
            # í•œê¸€ í°íŠ¸ê°€ ë“±ë¡ëœ ê²½ìš°
            from reportlab.lib.styles import ParagraphStyle
            
            korean_title = ParagraphStyle(
                'KoreanTitle',
                parent=styles['Title'],
                fontName='Korean',
                fontSize=16,
                spaceAfter=20
            )
            
            korean_heading = ParagraphStyle(
                'KoreanHeading',
                parent=styles['Heading2'],
                fontName='Korean',
                fontSize=12,
                spaceAfter=10
            )
            
            korean_normal = ParagraphStyle(
                'KoreanNormal',
                parent=styles['Normal'],
                fontName='Korean',
                fontSize=10,
                spaceAfter=8
            )
        else:
            # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì‚¬ìš©
            korean_title = styles['Title']
            korean_heading = styles['Heading2']
            korean_normal = styles['Normal']
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì‚¬ìš©
        styles = getSampleStyleSheet()
        korean_title = styles['Title']
        korean_heading = styles['Heading2']
        korean_normal = styles['Normal']
    
    story = []
    
    # ì œëª© ì¶”ê°€
    title = Paragraph(f"<b>ê¸°ì‚¬ ì£¼ì œ: {topic}</b>", korean_title)
    story.append(title)
    story.append(Spacer(1, 20))
    
    # ë¬¸ì¥ê³¼ ë²ˆì—­ ì¶”ê°€
    for i, (sentence, translation) in enumerate(zip(sentences, translations), 1):
        # ë¬¸ì¥ ë²ˆí˜¸
        story.append(Paragraph(f"<b>ë¬¸ì¥ {i}:</b>", korean_heading))
        # ì›ë¬¸
        story.append(Paragraph(f"<b>ì›ë¬¸:</b> {sentence}", korean_normal))
        # ë²ˆì—­
        story.append(Paragraph(f"<b>ë²ˆì—­:</b> {translation}", korean_normal))
        story.append(Spacer(1, 15))
    
    doc.build(story)
    buffer.seek(0)
    return buffer

# ====== Session State ì´ˆê¸°í™” ======
if 'analysis_data' not in st.session_state:
    st.session_state.analysis_data = None
if 'current_url' not in st.session_state:
    st.session_state.current_url = ""

# ====== UI êµ¬ì„± ======
url_input = st.text_input("ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”:", value=st.session_state.current_url)

if st.button("ê¸°ì‚¬ ë¶„ì„ ì‹œì‘"):
    if not url_input:
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”!")
    else:
        # 1ï¸âƒ£ ê¸°ì‚¬ í¬ë¡¤ë§
        loader = WebBaseLoader(url_input)
        docs = loader.load()
        text = docs[0].page_content

        # 2ï¸âƒ£ í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ëª¨ë“  ë°ì´í„° ì¶”ì¶œ
        analysis_prompt = PromptTemplate(
            input_variables=["text"],
            template=(
                "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ë§Œ ì°¾ì•„ì„œ ë¶„ì„í•´ì¤˜. "
                "ê´‘ê³ , ë©”ë‰´, ëŒ“ê¸€, ê´€ë ¨ê¸°ì‚¬ ëª©ë¡, ì‚¬ì´íŠ¸ ë„¤ë¹„ê²Œì´ì…˜ ë“±ì€ ë¬´ì‹œí•˜ê³  ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ë§Œ ì²˜ë¦¬í•´ì¤˜.\n\n"
                
                "ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì¤˜:\n"
                "{{\n"
                '  "topic": "ê¸°ì‚¬ ì£¼ì œ (í•œêµ­ì–´ 20ì ì´ë‚´)",\n'
                '  "sentences": ["ì™¸êµ­ì–´ ë¬¸ì¥1", "ì™¸êµ­ì–´ ë¬¸ì¥2", "ì™¸êµ­ì–´ ë¬¸ì¥3", ...],\n'
                '  "translations": ["í•œêµ­ì–´ ë²ˆì—­1", "í•œêµ­ì–´ ë²ˆì—­2", "í•œêµ­ì–´ ë²ˆì—­3", ...],\n'
                '  "words": ["ì™¸êµ­ì–´ë‹¨ì–´1", "ì™¸êµ­ì–´ë‹¨ì–´2", "ì™¸êµ­ì–´ë‹¨ì–´3", ...],\n'
                '  "word_meanings": ["í•œêµ­ì–´ëœ»1", "í•œêµ­ì–´ëœ»2", "í•œêµ­ì–´ëœ»3", ...],\n'
                '  "word_sentences": ["í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜ì˜¨ ë¬¸ì¥1", "í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜ì˜¨ ë¬¸ì¥2", "í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜ì˜¨ ë¬¸ì¥3", ...]\n'
                "}}\n\n"
                
                "ì¤‘ìš”í•œ ê·œì¹™:\n"
                "1. sentencesì™€ translationsëŠ” ë°˜ë“œì‹œ ê°™ì€ ê°œìˆ˜ì—¬ì•¼ í•¨\n"
                "2. words, word_meanings, word_sentencesëŠ” ë°˜ë“œì‹œ ê°™ì€ ê°œìˆ˜ì—¬ì•¼ í•¨ (í•œ ë¬¸ì¥ ë‹¹ 2ë‹¨ì–´)\n"
                "3. sentencesëŠ” ì™¸êµ­ì–´ ì›ë¬¸, translationsëŠ” í•œêµ­ì–´ ë²ˆì—­\n"
                "4. wordsëŠ” ì™¸êµ­ì–´ ë‹¨ì–´, word_meaningsëŠ” í•œêµ­ì–´ ëœ»\n"
                "5. word_sentencesëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ì‹¤ì œë¡œ ë‚˜ì˜¨ ë¬¸ì¥ (sentences ë°°ì—´ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ)\n"
                "6. í•´ë‹¹ ì–¸ì–´ì˜ ê´€ì‚¬, ì ‘ì†ì‚¬, ëŒ€ëª…ì‚¬, ê³ ìœ ëª…ì‚¬(ì‚¬ëŒ ì´ë¦„, ë‚˜ë¼ëª… ë“±)ëŠ” ì œì™¸(ì¤‘ìš”)\n"
                "7. ë°°ì—´ ê°œìˆ˜ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ê³  ì •í™•íˆ ë§ì¶°ì„œ ì¶œë ¥\n\n"
                
                "í…ìŠ¤íŠ¸:\n{text}"
            )
        )
        
        chain = LLMChain(llm=llm, prompt=analysis_prompt)
        
        with st.spinner("ê¸°ì‚¬ë¥¼ ë¶„ì„ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)"):
            result = chain.run(text)
        
        try:
            # JSON íŒŒì‹±
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
            if "```json" in result:
                json_start = result.find("```json") + 7
                json_end = result.find("```", json_start)
                json_str = result[json_start:json_end].strip()
            elif "{" in result and "}" in result:
                json_start = result.find("{")
                json_end = result.rfind("}") + 1
                json_str = result[json_start:json_end]
            else:
                json_str = result
            
            data = json.loads(json_str)
            
            # ë°ì´í„° ì¶”ì¶œ
            topic = data.get("topic", "ì£¼ì œ ì—†ìŒ")
            sentences = data.get("sentences", [])
            translations = data.get("translations", [])
            words = data.get("words", [])
            word_meanings = data.get("word_meanings", [])
            word_sentences = data.get("word_sentences", [])
            
            # ë°°ì—´ ê¸¸ì´ ê²€ì¦ ë° ë³´ì •
            if len(words) != len(word_meanings) or len(words) != len(word_sentences):
                st.warning(f"âš ï¸ ë‹¨ì–´ ë°°ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜ ê°ì§€: ë‹¨ì–´ {len(words)}ê°œ, ëœ» {len(word_meanings)}ê°œ, ë¬¸ì¥ {len(word_sentences)}ê°œ")
                
                # ê°€ì¥ ì§§ì€ ë°°ì—´ì— ë§ì¶°ì„œ ìë¥´ê¸°
                min_length = min(len(words), len(word_meanings), len(word_sentences))
                words = words[:min_length]
                word_meanings = word_meanings[:min_length]
                word_sentences = word_sentences[:min_length]
                
                st.info(f"âœ… ë°°ì—´ ê¸¸ì´ë¥¼ {min_length}ê°œë¡œ ë§ì·„ìŠµë‹ˆë‹¤.")
            
            if len(sentences) != len(translations):
                st.warning(f"âš ï¸ ë¬¸ì¥ ë°°ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜ ê°ì§€: ë¬¸ì¥ {len(sentences)}ê°œ, ë²ˆì—­ {len(translations)}ê°œ")
                
                # ë” ì§§ì€ ë°°ì—´ì— ë§ì¶°ì„œ ìë¥´ê¸°
                min_length = min(len(sentences), len(translations))
                sentences = sentences[:min_length]
                translations = translations[:min_length]
                
                st.info(f"âœ… ë°°ì—´ ê¸¸ì´ë¥¼ {min_length}ê°œë¡œ ë§ì·„ìŠµë‹ˆë‹¤.")
            
            # Session Stateì— ë¶„ì„ ê²°ê³¼ ì €ì¥
            st.session_state.analysis_data = {
                'topic': topic,
                'sentences': sentences,
                'translations': translations,
                'words': words,
                'word_meanings': word_meanings,
                'word_sentences': word_sentences,
                'difficulty_level': calculate_difficulty(sentences)
            }
            st.session_state.current_url = url_input
            
        except json.JSONDecodeError as e:
            st.error("ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            st.write("ì›ë³¸ ê²°ê³¼:")
            st.code(result)
        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.write("ì›ë³¸ ê²°ê³¼:")
            st.code(result)

# ====== ë¶„ì„ ê²°ê³¼ í‘œì‹œ (Session State ì‚¬ìš©) ======
if st.session_state.analysis_data:
    data = st.session_state.analysis_data
    
    # ê¸°ì‚¬ ì£¼ì œ í‘œì‹œ
    st.info(f"ğŸ“° **ê¸°ì‚¬ ì£¼ì œ**: {data['topic']}")
    
    # í†µê³„ í‘œì‹œ
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ“ ë¬¸ì¥ ìˆ˜", f"{len(data['sentences'])}ê°œ")
    with col2:
        st.metric("ğŸ“š ë‹¨ì–´ ìˆ˜", f"{len(data['words'])}ê°œ")
    with col3:
        color = "ğŸŸ¢" if data['difficulty_level'] in ["A1", "A2"] else "ğŸŸ¡" if data['difficulty_level'] in ["B1", "B2"] else "ğŸ”´"
        st.metric("ğŸ¯ ë‚œì´ë„", f"{color} {data['difficulty_level']}")
    
    st.divider()
    
    # íƒ­ìœ¼ë¡œ í˜ì´ì§€ êµ¬ë¶„
    tab1, tab2 = st.tabs(["ğŸ“– ë¬¸ì¥ë³„ ë²ˆì—­", "ğŸ“š ë‹¨ì–´ì¥"])
    
    with tab1:
        st.subheader("ë¬¸ì¥ë³„ ë²ˆì—­")
        
        # ë¬¸ì¥ê³¼ ë²ˆì—­ í‘œì‹œ
        for i, (sentence, translation) in enumerate(zip(data['sentences'], data['translations']), 1):
            st.write(f"**ë¬¸ì¥ {i}:**")
            st.write(f"**ì›ë¬¸:** {sentence}")
            st.write(f"**ë²ˆì—­:** {translation}")
            st.write("---")
        
        # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        if data['sentences'] and data['translations']:
            try:
                pdf_buffer = create_pdf(data['sentences'], data['translations'], data['topic'])
                st.download_button(
                    label="ğŸ“„ ë¬¸ì¥ë³„ ë²ˆì—­ PDF ë‹¤ìš´ë¡œë“œ",
                    data=pdf_buffer.getvalue(),
                    file_name=f"sentences_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                    mime="application/pdf",
                    key="pdf_download"
                )
            except Exception as e:
                st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    with tab2:
        st.subheader("ë‹¨ì–´ì¥")
        
        # ë‹¨ì–´ì¥ í…Œì´ë¸” ìƒì„±
        if data['words'] and data['word_meanings'] and data['word_sentences']:
            vocab_df = pd.DataFrame({
                'ë‹¨ì–´': data['words'],
                'ëœ»': data['word_meanings'],
                'ì˜ˆë¬¸': data['word_sentences']
            })
            
            # í…Œì´ë¸” í‘œì‹œ
            st.dataframe(vocab_df, use_container_width=True)
            
            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (UTF-8 ì¸ì½”ë”©)
            csv = vocab_df.to_csv(index=False, encoding='utf-8')
            st.download_button(
                label="ğŸ“¥ ë‹¨ì–´ì¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv.encode('utf-8'),
                file_name=f"vocabulary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime='text/csv',
                key="csv_download"
            )
            
            st.success(f"âœ… ì´ {len(data['words'])}ê°œì˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
        else:
            st.warning("ë‹¨ì–´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìƒˆ ë¶„ì„ ë²„íŠ¼
    if st.button("ğŸ”„ ìƒˆë¡œìš´ ê¸°ì‚¬ ë¶„ì„í•˜ê¸°", type="secondary"):
        st.session_state.analysis_data = None
        st.session_state.current_url = ""
        st.rerun()